{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eatd/Awesome-AI-Data-Guided-Projects/blob/main/GRPO_FineTuning_DeepSeek_R1_Distill_Qwen_1_5B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b71edb4",
      "metadata": {
        "id": "3b71edb4"
      },
      "source": [
        "# Applying GRPO to DeepSeek-R1-Distill-Qwen-1.5B with LIMO Dataset\n",
        "\n",
        "This notebook provides a step-by-step tutorial for applying **Generalized Reinforcement Policy Optimization (GRPO)** to the distilled model **DeepSeek-R1-Distill-Qwen-1.5B** using the high-quality LIMO dataset. We will cover:\n",
        "\n",
        "1. **Setup & Installation** – Installing dependencies and verifying GPU availability.\n",
        "2. **Model & Dataset Preparation** – Loading the model, tokenizer, and dataset, and formatting prompts.\n",
        "3. **Reinforcement Learning Fine-Tuning (GRPO)** – Implementing a simplified GRPO training loop, including reward computation and KL regularization.\n",
        "4. **Evaluation & Performance Metrics** – Demonstrating how to evaluate the fine-tuned model on benchmark tasks.\n",
        "5. **Hyperparameter Ablations & Future Directions** – Discussion on tuning and potential improvements.\n",
        "\n",
        "Let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b55f4e70",
      "metadata": {
        "id": "b55f4e70"
      },
      "source": [
        "## 1. Setup & Installation\n",
        "\n",
        "We first install the necessary libraries including PyTorch, Hugging Face Transformers, TRL (for reinforcement learning), the Datasets library, and bitsandbytes for 8-bit optimization. Then, we verify that a GPU is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7f4e7711",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f4e7711",
        "outputId": "7a267804-31d2-4c2b-a513-7f0be38acf3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.48.2\n",
            "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement trl==0.15.0.dev0 (from versions: 0.0.1, 0.0.2, 0.0.3, 0.1.0, 0.2.0, 0.2.1, 0.3.0, 0.3.1, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7, 0.5.0, 0.6.0, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.7.9, 0.7.10, 0.7.11, 0.8.0, 0.8.1, 0.8.2, 0.8.3, 0.8.4, 0.8.5, 0.8.6, 0.9.2, 0.9.3, 0.9.4, 0.9.6, 0.10.1, 0.11.0, 0.11.1, 0.11.2, 0.11.3, 0.11.4, 0.12.0, 0.12.1, 0.12.2, 0.13.0, 0.14.0, 0.15.0, 0.15.1, 0.15.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for trl==0.15.0.dev0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.48.2 trl==0.15.0.dev0 datasets bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c6b72a2c",
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6b72a2c",
        "outputId": "8836b33b-9b4c-4d5b-f233-32edd536ca82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.5.1+cu124\n",
            "GPU detected: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "import torch\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    print(\"GPU detected:\", device_name)\n",
        "    # Enable TF32 for faster matrix multiplication on supported GPUs\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "else:\n",
        "    print(\"No GPU found. Please enable a GPU runtime for training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb314410",
      "metadata": {
        "id": "eb314410"
      },
      "source": [
        "## 2. Model & Dataset Preparation\n",
        "\n",
        "We now load the **DeepSeek-R1-Distill-Qwen-1.5B** model and its tokenizer from Hugging Face, and load the LIMO dataset. The dataset consists of high-quality reasoning samples with a `question`, a detailed `solution`, and the final `answer`.\n",
        "\n",
        "We also define a helper function `format_prompt` that formats the question into a prompt instructing the model to output a reasoning chain and final answer using the tags `<think>` and `<answer>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e3f3f80e",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "e3f3f80e",
        "outputId": "0306a393-668a-45db-aef0-36e10693432c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    453\u001b[0m             )\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-67c5076e-7ec9c5917076061e43ccde97;57f8624d-f11a-4850-bf45-23fc6ad9e8d7)\n\nRepository Not Found for url: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid credentials in Authorization header",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ee2e62e49ace>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m model = AutoModelForCausalLM.from_pretrained(\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m         ) from e\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        "    # Uncomment the following line if the model requires custom code\n",
        "    # trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Quick test generation\n",
        "prompt_test = \"What is the capital of France?\"\n",
        "inputs_test = tokenizer(prompt_test, return_tensors=\"pt\").to(model.device)\n",
        "outputs_test = model.generate(**inputs_test, max_new_tokens=10)\n",
        "print(\"Test output:\", tokenizer.decode(outputs_test[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b8f4d2e",
      "metadata": {
        "id": "2b8f4d2e"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the LIMO dataset\n",
        "dataset = load_dataset(\"GAIR/LIMO\")\n",
        "train_data = dataset[\"train\"]\n",
        "print(\"Total training samples:\", len(train_data))\n",
        "\n",
        "# Display a sample\n",
        "sample = train_data[0]\n",
        "print(\"Question:\", sample[\"question\"])\n",
        "print(\"Solution (excerpt):\", sample[\"solution\"][:100] + \"...\")\n",
        "print(\"Answer:\", sample[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaeb2a11",
      "metadata": {
        "id": "eaeb2a11"
      },
      "outputs": [],
      "source": [
        "def format_prompt(question):\n",
        "    \"\"\"\n",
        "    Format the prompt to instruct the model to output a chain-of-thought and final answer.\n",
        "    \"\"\"\n",
        "    instruction = (\n",
        "        \"Solve the following problem step by step, then give the final answer. \"\n",
        "        \"Format your response as: <think>[reasoning]</think><answer>[final answer]</answer>.\"\n",
        "    )\n",
        "    return f\"{instruction}\\nQuestion: {question}\\nSolution:\"\n",
        "\n",
        "# Test the formatting\n",
        "formatted_prompt = format_prompt(sample[\"question\"])\n",
        "print(formatted_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0fce501",
      "metadata": {
        "id": "f0fce501"
      },
      "source": [
        "## 3. Reinforcement Learning Fine-Tuning (GRPO)\n",
        "\n",
        "In this section, we implement a simplified GRPO training loop. The main steps include:\n",
        "\n",
        "- **Sampling:** For each prompt, we generate multiple outputs (a group) from the model.\n",
        "- **Reward Scoring:** Compute a reward for each output based on answer accuracy and proper formatting.\n",
        "- **Advantage Calculation:** Compute the advantage by comparing each reward to the group average.\n",
        "- **Policy Optimization:** Update the model weights using the advantage-weighted log-likelihood loss along with a KL divergence penalty to keep the model close to the reference (base) policy.\n",
        "\n",
        "We use a default learning rate of `1e-6`, group size of 7, and a KL weight `β = 0.04`. We also set up an optimizer that supports 8-bit parameters (via bitsandbytes) for memory efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0b7a87d",
      "metadata": {
        "cellView": "form",
        "id": "d0b7a87d"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import math\n",
        "from transformers import AdamW  # Standard AdamW\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 1e-6\n",
        "tokens_per_generation = 4096  # Maximum tokens per generation (can be ablated)\n",
        "group_size = 7\n",
        "beta = 0.04\n",
        "\n",
        "# Initialize the 8-bit AdamW optimizer (using bitsandbytes)\n",
        "import bitsandbytes as bnb\n",
        "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Optionally, use standard 32-bit AdamW:\n",
        "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Clone the initial model to serve as the reference for KL divergence\n",
        "from transformers import AutoModelForCausalLM\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(model.device)\n",
        "ref_model.eval()\n",
        "for param in ref_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "def reward_function(question, generated_text, true_answer):\n",
        "    \"\"\"\n",
        "    A simple rule-based reward:\n",
        "      - +0.1 bonus if output contains both <think> and <answer> tags\n",
        "      - +1.0 if the extracted answer matches the true answer\n",
        "      - Small penalty if no answer is extracted\n",
        "    \"\"\"\n",
        "    answer = None\n",
        "    if \"<answer>\" in generated_text and \"</answer>\" in generated_text:\n",
        "        start = generated_text.index(\"<answer>\") + len(\"<answer>\")\n",
        "        end = generated_text.index(\"</answer>\")\n",
        "        answer = generated_text[start:end].strip()\n",
        "    else:\n",
        "        # Fallback: take the last token as the answer\n",
        "        answer = generated_text.strip().split()[-1]\n",
        "\n",
        "    reward = 0.0\n",
        "    # Bonus for proper formatting\n",
        "    if \"<think>\" in generated_text and \"</think>\" in generated_text and \"<answer>\" in generated_text and \"</answer>\" in generated_text:\n",
        "        reward += 0.1\n",
        "\n",
        "    # Reward based on answer accuracy\n",
        "    if answer is not None:\n",
        "        pred_ans = answer.strip().strip('.')\n",
        "        true_ans = str(true_answer).strip().strip('.')\n",
        "        if pred_ans == true_ans:\n",
        "            reward += 1.0\n",
        "    else:\n",
        "        reward -= 0.1\n",
        "\n",
        "    return reward\n",
        "\n",
        "print(\"Optimizer and reward function set up.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c51b0ea",
      "metadata": {
        "id": "3c51b0ea"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "model.train()\n",
        "max_train_steps = 2  # Demo steps; in practice, use many more steps\n",
        "grad_accum_steps = 8  # Effective batch: grad_accum_steps * group_size\n",
        "\n",
        "# Shuffle training indices\n",
        "indices = list(range(len(train_data)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "step = 0\n",
        "optimizer.zero_grad()\n",
        "\n",
        "for idx in indices[: max_train_steps * grad_accum_steps]:\n",
        "    question = train_data[idx][\"question\"]\n",
        "    true_answer = train_data[idx][\"answer\"]\n",
        "    prompt = format_prompt(question)\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    # Generate a group of outputs\n",
        "    generated_texts = []\n",
        "    for _ in range(group_size):\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=200,  # For demo; in practice, use tokens_per_generation\n",
        "            do_sample=True,\n",
        "            temperature=1.0,\n",
        "            eos_token_id=tokenizer.convert_tokens_to_ids(\"</answer>\")\n",
        "        )\n",
        "        generated = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "        generated_texts.append(generated)\n",
        "\n",
        "    # Compute rewards and advantages\n",
        "    rewards = [reward_function(question, text, true_answer) for text in generated_texts]\n",
        "    baseline = sum(rewards) / len(rewards)\n",
        "    advantages = [r - baseline for r in rewards]\n",
        "\n",
        "    # Compute policy loss\n",
        "    policy_loss = 0.0\n",
        "    for text, adv in zip(generated_texts, advantages):\n",
        "        full_text = prompt + text\n",
        "        enc = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
        "        labels = enc.input_ids.clone()\n",
        "        labels[:, :input_ids.shape[1]] = -100  # Mask prompt tokens from loss\n",
        "        out = model(**enc, labels=labels)\n",
        "        # Multiply the average loss by the number of output tokens\n",
        "        policy_loss += adv * (out.loss * labels[:, input_ids.shape[1]:].numel())\n",
        "    policy_loss = policy_loss / group_size\n",
        "\n",
        "    # Approximate KL divergence loss\n",
        "    kl_loss = 0.0\n",
        "    for text in generated_texts:\n",
        "        full_text = prompt + text\n",
        "        enc = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
        "        labels = enc.input_ids.clone()\n",
        "        labels[:, :input_ids.shape[1]] = -100\n",
        "        with torch.no_grad():\n",
        "            curr_out = model(**enc, labels=labels)\n",
        "            ref_out = ref_model(**enc, labels=labels)\n",
        "        curr_nll = curr_out.loss * labels[:, input_ids.shape[1]:].numel()\n",
        "        ref_nll = ref_out.loss * labels[:, input_ids.shape[1]:].numel()\n",
        "        kl_loss += (curr_nll - ref_nll) / labels[:, input_ids.shape[1]:].numel()\n",
        "    kl_loss = kl_loss / group_size\n",
        "\n",
        "    total_loss = policy_loss + beta * kl_loss\n",
        "    total_loss.backward()\n",
        "\n",
        "    if (idx + 1) % grad_accum_steps == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        step += 1\n",
        "        print(f\"Step {step}: policy_loss={policy_loss.item():.4f}, kl_loss={kl_loss.item():.4f}, rewards={rewards}\")\n",
        "        if step >= max_train_steps:\n",
        "            break\n",
        "\n",
        "model.eval()\n",
        "print(\"Training demo completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e36ea3a",
      "metadata": {
        "id": "7e36ea3a"
      },
      "source": [
        "## 4. Evaluation & Performance Metrics\n",
        "\n",
        "After fine-tuning, we evaluate the model on reasoning benchmarks (e.g., AIME24, GPQA, MATH-500). In this demo, we show an evaluation example for one benchmark.\n",
        "\n",
        "The process involves:\n",
        "\n",
        "- Formatting the prompt as during training.\n",
        "- Generating an answer using greedy decoding.\n",
        "- Extracting the answer using the `<answer>` tags and comparing it with the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d86ce3",
      "metadata": {
        "id": "97d86ce3"
      },
      "outputs": [],
      "source": [
        "# Example evaluation for a benchmark (e.g., AIME24)\n",
        "# For illustration, let's assume we have lists of questions and true answers\n",
        "\n",
        "aime_questions = [\n",
        "    \"If x + y = 10 and x - y = 2, what is the value of x?\",\n",
        "    \"Compute the area of a circle with radius 7.\"\n",
        "]\n",
        "aime_answers = [\n",
        "    \"6\",  # x = 6\n",
        "    \"153.938\"  # Approximate area (could be rounded)\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "for question, true_answer in zip(aime_questions, aime_answers):\n",
        "    prompt = format_prompt(question)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=512, temperature=0.0)  # Greedy decoding\n",
        "    output_text = tokenizer.decode(output_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    if \"<answer>\" in output_text and \"</answer>\" in output_text:\n",
        "        ans = output_text.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n",
        "    else:\n",
        "        ans = output_text.strip().split()[-1]\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Predicted Answer: {ans}\")\n",
        "    print(f\"True Answer: {true_answer}\\n\")\n",
        "\n",
        "    if str(ans).strip().strip('.') == str(true_answer).strip().strip('.'):\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / len(aime_questions) * 100\n",
        "print(f\"AIME24 Accuracy: {accuracy:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7e82731",
      "metadata": {
        "id": "d7e82731"
      },
      "source": [
        "## 5. Hyperparameter Ablations & Future Directions\n",
        "\n",
        "### Hyperparameter Ablations\n",
        "\n",
        "Key hyperparameters that can be tuned include:\n",
        "\n",
        "- **Learning Rate:** Our default is `1e-6`, but values like `2e-6`, `4e-6`, or `8e-6` may be experimented with.\n",
        "- **Group Size:** Number of outputs per prompt (default is 7). Increasing this (e.g., 14, 28, or 56) can provide a more robust reward baseline but at higher computational cost.\n",
        "- **KL Weight (β):** Default is `0.04`. Lower values (e.g., 0.01 or 0.001) allow the model more freedom to explore but may risk divergence.\n",
        "\n",
        "### Future Directions\n",
        "\n",
        "- **Refining the Reward Function:** Improve extraction of the final answer and consider partial rewards for nearly correct outputs.\n",
        "- **Adaptive KL Penalty:** Use adaptive techniques to adjust β based on the observed KL divergence during training.\n",
        "- **Scaling Up:** Experiment with larger models or longer generation tokens to fully exploit the reasoning capabilities.\n",
        "- **Distillation vs. Pretrained Models:** Compare training outcomes when starting from a distilled model versus a base pretrained model.\n",
        "\n",
        "This concludes our step-by-step guide. Happy fine-tuning!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "GRPO_FineTuning_DeepSeek-R1-Distill-Qwen-1.5B.ipynb",
      "provenance": [],
      "history_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}